{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import os,time\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import math\n",
    "import utils\n",
    "from utils import Scoring\n",
    "import matplotlib.pyplot as plt\n",
    "from dataLoader import reshape_folds\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.backends import cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      fbank\n",
       "1                  log-fbank\n",
       "2                    mfcc_26\n",
       "3                    mfcc_13\n",
       "4            fbank_log-fbank\n",
       "5              fbank_mfcc_13\n",
       "6          log-fbank_mfcc_13\n",
       "7    fbank_log-fbank_mfcc_13\n",
       "8                  mfcc_13_d\n",
       "9                 mfcc_13_dd\n",
       "Name: feature, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel (r'../logs/pretrained.xlsx')\n",
    "input_features = df[\"feature\"].iloc[:]\n",
    "input_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Results From Two Different Input Features\n",
    "\n",
    "Select two features from the upper list to observe the validation result and put them in the \"Selected Feature List\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fbank --- EPOCH ---- ( 25 )----\n",
      "Validation loss -  0.5178222060203552\n",
      "TN: 122 FP: 24 FN: 25 TP: 113\n",
      "Sensitivity: 0.819 Specificity: 0.836 Precision: 0.825 F1: 0.822 MACC 0.827 Accuracy 0.827\n",
      "\n",
      "log-fbank --- EPOCH ---- ( 64 )----\n",
      "Validation loss -  0.5149890184402466\n",
      "TN: 121 FP: 25 FN: 19 TP: 119\n",
      "Sensitivity: 0.862 Specificity: 0.829 Precision: 0.826 F1: 0.844 MACC 0.846 Accuracy 0.845\n"
     ]
    }
   ],
   "source": [
    "feature_name = []\n",
    "predictions = []\n",
    "\n",
    "for feature in [\"fbank\", \"log-fbank\"]: # Selected Feature List\n",
    "    \n",
    "    index = [i for i, feat in enumerate(input_features) if(feat==feature)][0]\n",
    "\n",
    "    class paramClass():\n",
    "        def __init__(self):\n",
    "            self.epoch = df[\"epoch\"][index]\n",
    "            self.batch_size = df[\"batch_size\"][index]\n",
    "            self.inp = df[\"inp\"][index]\n",
    "            self.base_lr = df[\"base_lr\"][index]\n",
    "            self.max_lr = df[\"max_lr\"][index]\n",
    "            self.step_size = df[\"step_size\"][index]\n",
    "            self.checkpoint = '../logs/' + self.inp + \"/checkpoints/\"\n",
    "    param = paramClass()\n",
    "\n",
    "    data_file = \"../data/feature/train_val_npzs/\" + param.inp + \"_train_val_data.npz\"\n",
    "    data = np.load(data_file)\n",
    "\n",
    "    x_val = data['x_val_mfcc']\n",
    "    y_val = data['y_val']\n",
    "    val_parts = data['val_parts']\n",
    "    val_wav_files = data['val_wav_files']\n",
    "\n",
    "    x_val = x_val.reshape(x_val.shape[0],x_val.shape[1],x_val.shape[2], 1)\n",
    "\n",
    "    #--- Reshape the folds\n",
    "    _, [y_val] = reshape_folds([], [y_val])\n",
    "\n",
    "    from keras.utils import to_categorical\n",
    "    y_val = to_categorical(y_val, num_classes = 2)\n",
    "\n",
    "    from model import resnet_extractor, abn_classifier\n",
    "    nodes = [16, 32, 64, 128]\n",
    "    num_layers = 2\n",
    "    model_fe = resnet_extractor(1, None, param.inp, nodes, num_layers).cuda()\n",
    "    model_abn = abn_classifier(model_fe.out_features, 2, dropout = None).cuda()\n",
    "\n",
    "    e = param.epoch - 1\n",
    "    checkpoint_fe = param.checkpoint + str(e+1) + \"_saved_fe_model.pth.tar\"\n",
    "    model_fe.load_state_dict(torch.load(checkpoint_fe)[\"state_dict\"])\n",
    "    checkpoint_abn = param.checkpoint + str(e+1) + \"_saved_cls_model.pth.tar\"\n",
    "    model_abn.load_state_dict(torch.load(checkpoint_abn)[\"state_dict\"])\n",
    "\n",
    "    epoch_loss = torch.load(checkpoint_fe)[\"loss\"]\n",
    "    val_loss_load = torch.load(checkpoint_fe)[\"val_loss\"]\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"\\n\" + feature + \" --- EPOCH ---- ( \" + str(e+1) + \" )----\")\n",
    "\n",
    "    model_fe.eval()\n",
    "    model_abn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cls_pred = None\n",
    "        cls_val = None\n",
    "        total_features = np.array([])\n",
    "        epoch_val_loss = 0\n",
    "        s = 0\n",
    "        for i, part in enumerate(val_parts):\n",
    "            x,y = torch.from_numpy(x_val[s:s+part]),torch.from_numpy(y_val[s:s+part])\n",
    "            s = s + part\n",
    "\n",
    "            if(len(x) == 0): # If no bits are found\n",
    "                continue\n",
    "\n",
    "            x,y = Variable(x),Variable(y)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "\n",
    "            x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "            y = torch.tensor(np.array(y).reshape(-1, 2)).cuda().float()\n",
    "\n",
    "            features = model_fe(x)\n",
    "            cls = model_abn(features)\n",
    "\n",
    "            val_loss = loss_fn(cls, torch.argmax(y,dim=1))\n",
    "\n",
    "            if(i==0): total_features = features.cpu().numpy()\n",
    "            else: total_features = np.concatenate((total_features, features.cpu().numpy()))\n",
    "\n",
    "            if(cls_pred is None):\n",
    "                cls_pred = cls\n",
    "                cls_val = y\n",
    "            else:\n",
    "                cls_pred = torch.cat((cls_pred,cls))\n",
    "                cls_val = torch.cat((cls_val,y))\n",
    "\n",
    "            epoch_val_loss = epoch_val_loss + val_loss\n",
    "\n",
    "        epoch_val_loss = epoch_val_loss/len(val_parts)\n",
    "\n",
    "        print(\"Validation loss - \", str(epoch_val_loss.item()))\n",
    "\n",
    "    domain = np.asarray(val_wav_files).reshape((-1, 1))\n",
    "    score_log = Scoring(e + 1)\n",
    "    _, true, pred, _ = score_log.log_score(cls_pred, cls_val, val_parts, \n",
    "            y_domain = domain, list_out=True)\n",
    "    \n",
    "    feature_name.append(feature)\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McNemer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "def McnemerStats(true,pred1,pred2,threshold = 0.05, lab1='model 1', lab2='model 2'):\n",
    "    \"\"\"\n",
    "    true: true labels of the data. \n",
    "    pred1: predicted values from model 1 ## generated from block 8\n",
    "    pred2: predicted values from model 2 ## generated from block 8\n",
    "    \"\"\"\n",
    "    pred1 = 1*(np.asarray(pred1) > 0.5)\n",
    "    pred2 = 1*(np.asarray(pred2) > .5)\n",
    "    true = np.asarray(true)\n",
    "    \n",
    "    mc_table = mcnemar_table(y_target=true, \n",
    "                   y_model1=pred1, \n",
    "                   y_model2=pred2)\n",
    "    if(np.min(mc_table)<25):\n",
    "        result = mcnemar(mc_table, exact=True)\n",
    "    else:\n",
    "        result = mcnemar(mc_table, exact=False, correction=True)\n",
    "    print('statistic=%.10f, p-value=%.10f' % (result.statistic, result.pvalue))\n",
    "    # interpret the p-value\n",
    "    alpha = threshold\n",
    "    if result.pvalue > alpha:\n",
    "        print('Same proportions of errors, models make similar error (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different proportions of errors, error rates are different (reject H0)')\n",
    "    return result.statistic,result.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic=18.0000000000, p-value=0.5327092552\n",
      "Same proportions of errors, models make similar error (fail to reject H0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18.0, 0.5327092552361133)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "McnemerStats(true,predictions[0],predictions[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kagg_p",
   "language": "python",
   "name": "kagg_p"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}